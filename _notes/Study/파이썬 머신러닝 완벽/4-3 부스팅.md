- 여러개의 약한 학습기를 순차적으로 학습-예측 함ㅡ로써 잘못 예측한 데이터 or 학습 트리에 가중치 부여하여 오류 개선
- 대표는 Ada boosting Gradient Boosting

### Ada Boosting

![[스크린샷 2024-01-23 오후 7.44.18.png]]
각각의 가중치 값을 부여함으로써... 결합

### GBM
가중치 업데이트를 Gradient Descent를 이용하는 방법

Loss 함수를 기반으로 예측 오류를 줄이는 방향으로 모델 업데이트.

##### 하이퍼 파라미터 & 튜닝
![[스크린샷 2024-01-23 오후 7.47.28.png]]
- Learning Rate는 0.01~0.02 사이 정도
- 

### XGBoost(사이킷런 기준)
- GBM의 단점 보완하는 모델
- 성능 / 유용도 때문에 자주 사용됨
	- GBM 대비 빠른 실행시간
	- CPU 병렬 처리, GPU 지원으로 빠름
- 규제
	- 오차오류만 줄이면 과적합 걸리기 쉬움 -> 규제
- Tree Pruning
	- 노드들을 제대로 역할 하는지 말단노드로부터 검증
- Early Stopping
	- 오류 감소가 특정 기간동안 안나오면 stop.
![[스크린샷 2024-01-23 오후 7.55.27.png]]

하이퍼 파라미터
![[스크린샷 2024-01-23 오후 8.08.20.png]]![[스크린샷 2024-01-23 오후 8.10.06.png]]
Early Stopping

더 이상 오류감소 효과가 없으면 멈춰버리는!
![[스크린샷 2024-01-23 오후 8.13.06.png]]